{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kalbefarma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3989659838e64dd7a24218bdae5c3e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "import shutil\n",
    "import textwrap\n",
    "from ast import literal_eval\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.indexes import GraphIndexCreator\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "import pdf2image\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "!mkdir ./KG\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-\" # Your own OpenAI Key \n",
    "\n",
    "chunk_size = 16000\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove section titles and figure descriptions from text\"\"\"\n",
    "    clean = \"\\n\".join([row for row in text.split(\"\\n\") if (len(row.split(\" \"))) > 3 and not (row.startswith(\"(a)\")) and not row.startswith(\"Figure\")])\n",
    "    return clean\n",
    "\n",
    "def truncate_text(text, max_tokens):\n",
    "    wrapper = textwrap.TextWrapper(width=max_tokens)\n",
    "    truncated_text = wrapper.wrap(text)\n",
    "    if len(truncated_text) > 0:\n",
    "        return truncated_text[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text, chunk_size):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    while start < len(text):\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "        end += chunk_size\n",
    "    return chunks\n",
    "\n",
    "def split_text_into_sentences(text, num_sentences):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    grouped_sentences = [sentences[i:i+num_sentences] for i in range(0, len(sentences), num_sentences)]\n",
    "    return grouped_sentences\n",
    "\n",
    "def flatten_list(nested_list):\n",
    "    flattened_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "\n",
    "def move_file(source_path, destination_path):\n",
    "\n",
    "    # Make sure the destination folder exists before moving the file\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "\n",
    "    try:\n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f\"File moved successfully from '{source_path}' to '{destination_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k-0613\")\n",
    "\n",
    "schema = {\n",
    "    \"properties\" : {\n",
    "        \"title\" : {\"type\" : \"string\"},\n",
    "        \"author\" : {\"type\" : \"string\"},\n",
    "    },\n",
    "    \"required\" : [\"title\"]\n",
    "}\n",
    "\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "\n",
    "pdf_path = glob.glob(\"/Users/kalbefarma/Documents/Python/Others/pdf/*.pdf\") # Change to your own PDF directory\n",
    "\n",
    "for pdf in tqdm(pdf_path):\n",
    "    try:\n",
    "        images = pdf2image.convert_from_path(pdf)\n",
    "\n",
    "        extracted_text = \"\"\n",
    "        for image in images[:-1]:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            text = clean_text(text)\n",
    "            extracted_text += text + \" \"\n",
    "\n",
    "        triples = []\n",
    "        textb1 = split_text(extracted_text, chunk_size)\n",
    "\n",
    "        df = pd.DataFrame(literal_eval(str(chain.run(textb1)[0]).replace(\"\\'\", \"\\\"\")), index=[0]).fillna('')\n",
    "        triples.append((df['title'][0], df['author'][0], 'Written by'))\n",
    "\n",
    "        splitted_text = split_text_into_sentences(extracted_text, 3)\n",
    "        index_creator = GraphIndexCreator(llm=OpenAI(temperature=0, model=\"text-davinci-003\"))\n",
    "\n",
    "\n",
    "        for text in splitted_text:\n",
    "            graph = index_creator.from_text(text)\n",
    "            triples.append(graph.get_triples())\n",
    "\n",
    "        flattenedL = flatten_list(triples)\n",
    "        kgL = pd.DataFrame(flattenedL).rename(columns={0:'Source', 1:'Target', 2:'Relation'})\n",
    "        fname = pdf.split('/')[-1].replace('.pdf', '')\n",
    "        kgL.to_csv(f'./KG/{fname}.csv', index=False)\n",
    "    except Exception as e:\n",
    "        move_file(pdf, \"./Unprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
